from __future__ import annotations
import csv
import hashlib
import os
import sys
import time
import random
import traceback
import re
from datetime import datetime, timezone, timedelta
from pathlib import Path
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

class Logger:
    @staticmethod
    def banner(msg: str):
        print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*60}")
        print(f" {msg}")
        print(f"{'='*60}{Colors.ENDC}\n")
    @staticmethod
    def section(msg: str):
        print(f"\n{Colors.CYAN}â¤ {Colors.BOLD}{msg}{Colors.ENDC}")
    @staticmethod
    def info(msg: str, label: str = "INFO"):
        print(f"{Colors.BLUE}[{label}]{Colors.ENDC} {msg}")
    @staticmethod
    def success(msg: str):
        print(f"{Colors.GREEN}[SUCCESS]{Colors.ENDC} {msg}")
    @staticmethod
    def warning(msg: str):
        print(f"::warning::{msg}")
        print(f"{Colors.WARNING}[WARN]{Colors.ENDC} {msg}")
    @staticmethod
    def error(msg: str):
        print(f"::error::{msg}")
        print(f"{Colors.FAIL}[ERROR]{Colors.ENDC} {msg}")
    @staticmethod
    def group(title: str):
        print(f"::group::{title}")
    @staticmethod
    def endgroup():
        print("::endgroup::")

def read_text_smart(p: Path) -> str:
    try:
        return p.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        Logger.info(f"UTF-8 failed, trying UTF-8-SIG for {p.name}", "ENCODING")
        return p.read_text(encoding="utf-8-sig")

def sha256_file(p: Path) -> str:
    h = hashlib.sha256()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            h.update(chunk)
    return h.hexdigest()

def extract_old_stats(readme_content: str) -> int:
    """å°è¯•ä»æ—§çš„ README ä¸­æå–ä¹‹å‰çš„è¡Œæ•°ï¼Œç”¨äºå¯¹æ¯”"""
    match = re.search(r'badge/quotes-(\d+)-', readme_content)
    return int(match.group(1)) if match else 0

def load_data(csv_path: Path) -> tuple[list[list[str]], dict]:
    stats = {"total_rows": 0, "valid_data_rows": 0, "malformed_rows": 0}
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV file not found at: {csv_path.resolve()}")
    Logger.info(f"Reading file: {csv_path}", "IO")
    raw_content = read_text_smart(csv_path)
    lines = [line for line in raw_content.splitlines() if line.strip()]
    stats["total_rows"] = len(lines)
    rows = []
    reader = csv.reader(lines)
    for i, row in enumerate(reader):
        clean_row = [cell.strip() for cell in row]
        if any(clean_row):
            rows.append(clean_row)
    return rows, stats

def build_readme_content(ctx: dict, sample: dict) -> str:
    repo = ctx['repo']
    branch = os.getenv('DEFAULT_BRANCH', 'main')
    diff_val = ctx['diff_count']
    if diff_val > 0:
        diff_display = f"**+{diff_val}**"
    elif diff_val < 0:
        diff_display = str(diff_val)
    else:
        diff_display = "-" 

    checksum_short = ctx['csv_sha'][:12] + "..."
    link_raw = ctx['links']['raw']
    link_jsd = f"https://cdn.jsdelivr.net/gh/{repo}@{branch}/quotes.csv"
    link_stat = f"https://cdn.statically.io/gh/{repo}/{branch}/quotes.csv"
    link_ghp = f"https://mirror.ghproxy.com/{link_raw}"
    b_quotes = make_badge("QUOTES", ctx['rows_count'], "4F46E5", "googledocs") 
    b_size   = make_badge("SIZE", f"{ctx['size_kb']} KB", "059669", "database")
    b_time   = make_badge("UPDATE", "TODAY", "BE185D", "clock")
    btn_raw_img = f"https://img.shields.io/badge/GitHub_Raw-Source_File-2ea44f?style=for-the-badge&logo=github&logoColor=white"
    btn_jsd_img = f"https://img.shields.io/badge/jsDelivr-Global_CDN-ff5627?style=for-the-badge&logo=jsdelivr&logoColor=white"
    btn_stat_img = f"https://img.shields.io/badge/Statically-Multi_CDN-7c3aed?style=for-the-badge&logo=serverless&logoColor=white"
    btn_ghp_img = f"https://img.shields.io/badge/ghproxy-Mirror_Proxy-f97316?style=for-the-badge&logo=googlecloud&logoColor=white"


    md = [
        "<!-- AUTO-GENERATED -->",
        '<div align="center">',
        "",
        "# ğŸ“œ Bonjourr Chinese Quotes",
        "<h3>ç²¾é€‰ä¸­æ–‡è¯­å½•æ•°æ®é›† Â· æ¯æ—¥è‡ªåŠ¨æ›´æ–°</h3>",
        "",
        f'<img src="{b_quotes}" height="28"> <img src="{b_size}" height="28"> <img src="{b_time}" height="28">',
        "",
        "<br/>",
        "",

        '<table width="800">',
        '<tr><td align="center">',
        "",
        "### â˜•ï¸ ä»Šæ—¥ä¸€è¨€ (Daily Quote)",
        "",
        f"<h2>â {sample['quote']} â</h2>",
        f'<p align="right">â€”â€” <b>{sample["author"] or "ä½šå"}</b></p>',
        "",
        "</td></tr>",
        "</table>",
        "",
        "</div>", 
        "",
        "<br/>",
        "",

        "## âš¡ï¸ å¿«é€Ÿæ¥å…¥ / Quick Access",
        "",
        "### ğŸŸ¢ å®˜æ–¹æº (Stable)",
        f"[![Raw]({btn_raw_img})]({link_raw})",
        "```url",
        link_raw,
        "```",
        "",
        "### ğŸš€ å…¨çƒåŠ é€Ÿ (Global CDNs)",
        "> æ¨èç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªè®¿é—®æ…¢ï¼Œå¯åˆ‡æ¢å¦ä¸€ä¸ªã€‚",
        "",
        "**1. jsDelivr** (æ¨èï¼šå¿«é€Ÿã€ç¼“å­˜å¼º)",
        f"[![jsd]({btn_jsd_img})]({link_jsd})",
        "```url",
        link_jsd,
        "```",
        "",
        "**2. Statically** (å¤‡é€‰ï¼šåŸºäº Cloudflare/Fastly å¤šäº‘åˆ†å‘)",
        f"[![stat]({btn_stat_img})]({link_stat})",
        "```url",
        link_stat,
        "```",
        "",
        "### ğŸŒ åŒºåŸŸé•œåƒ (Mirrors)",
        "> é’ˆå¯¹ç‰¹å®šå—é™ç½‘ç»œç¯å¢ƒä¼˜åŒ–",
        "",
        "**ghproxy**",
        f"[![ghp]({btn_ghp_img})]({link_ghp})",
        "```url",
        link_ghp,
        "```",
        "",

        "<details>",
        "<summary><strong>ğŸ Python è¯»å–æ•°æ®ç¤ºä¾‹ä»£ç  (Click to expand)</strong></summary>",
        "",
        "```python",
        "import pandas as pd",
        "",
        "# å®šä¹‰åŠ é€Ÿæºåˆ—è¡¨",
        f'urls = [',
        f'    "{link_jsd}",      # é¦–é€‰',
        f'    "{link_stat}",     # å¤‡é€‰',
        f'    "{link_raw}"       # å…œåº•',
        "]",
        "",
        "df = None",
        "for url in urls:",
        "    try:",
        "        print(f'æ­£åœ¨å°è¯•: {url} ...')",
        "        df = pd.read_csv(url)",
        "        print('âœ… åŠ è½½æˆåŠŸï¼')",
        "        break",
        "    except Exception:",
        "        continue",
        "",
        "if df is not None:",
        "    print(df.sample(1))",
        "else:",
        "    print('âŒ æ‰€æœ‰æºå‡æ— æ³•è¿æ¥')",
        "```",
        "</details>",
        "",
        "<br/>",
        "",

        "## ğŸ“Š æ•°æ®çœ‹æ¿ / Dashboard",
        "",
        f"> **æ›´æ–°æ—¥å¿—**: {ctx['gen_cn']} (UTC+8)",
        "",
        "| æŒ‡æ ‡ | å½“å‰æ•°å€¼ | è¾ƒæ˜¨æ—¥å˜åŒ– |",
        "| :--- | :--- | :--- |",
        f"| **æ€»è¯­å½•æ•°** | `{ctx['rows_count']}` | {diff_display} |",
        f"| **æ–‡ä»¶å®Œæ•´æ€§** | `{checksum_short}` | SHA-256 Checksum |",
        "",
        "---",
        '<div align="center">',
        "<sub>ğŸ¤– Generated by GitHub Actions | <a href='https://github.com/'>Star this repository</a></sub>",
        "</div>"
    ]
    return "\n".join(md)

def make_badge(label: str, message: str, color: str, icon: str = "") -> str:
    """ç”Ÿæˆ Shields.io 'for-the-badge' é£æ ¼çš„ç²¾ç¾å¾½ç« """
    label = label.replace(" ", "%20")
    message = str(message).replace(" ", "%20")
    url = f"https://img.shields.io/badge/{label}-{message}-{color}?style=for-the-badge&labelColor=24292e"
    if icon:
        url += f"&logo={icon}&logoColor=white"
    return url

def generate_step_summary(ctx: dict, diagnositcs: list[str]):
    """ç”Ÿæˆ GitHub Actions æ¼‚äº®çš„ Summary"""
    summary_path = os.getenv("GITHUB_STEP_SUMMARY")
    if not summary_path:
        return
    icon = "âœ…" if ctx['diff_count'] >= 0 else "âš ï¸"
    md = [
        f"## {icon} Generator Execution Report",
        "",
        "### ğŸ“Š Statistics Snapshot",
        "",
        "| Metric | Value | Change |",
        "| :--- | :--- | :--- |",
        f"| **Total Quotes** | **{ctx['rows_count']}** | {ctx['diff_count']:+d} |",
        f"| **File Size** | {ctx['size_kb']} KB | - |",
        f"| **Execution Time** | {ctx['exec_time']:.2f}s | - |",
        "",
        "### ğŸ” Diagnostics",
        ""
    ]
    if diagnositcs:
        md.append("```text")
        md.extend(diagnositcs)
        md.append("```")
    else:
        md.append("No warnings or errors detected. CSV structure looks good.")
    Path(summary_path).write_text("\n".join(md), encoding="utf-8")

def main():
    start_time = time.time()
    Logger.banner("STARTING README GENERATION JOB")
    Logger.section("Checking Environment")
    repo = os.getenv("GITHUB_REPOSITORY", "local/test")
    branch = os.getenv("DEFAULT_BRANCH", "main")
    csv_rel = os.getenv("QUOTES_CSV", "quotes.csv")
    csv_path = Path(csv_rel)
    readme_path = Path("README.md")
    Logger.info(f"Repo: {repo} | Branch: {branch}")

    old_row_count = 0
    if readme_path.exists():
        Logger.info("Reading existing README for history comparison...", "HISTORY")
        try:
            old_content = read_text_smart(readme_path)
            old_row_count = extract_old_stats(old_content)
        except:
            pass 
            
    Logger.section("Processing CSV Data")
    try:
        rows, load_stats = load_data(csv_path)
    except Exception as e:
        Logger.error(f"Failed to load CSV: {e}")
        return 1

    has_header = False
    header = []
    q_idx, a_idx = 1, 0 
    
    if len(rows) > 0:
        first_row = [c.lower().strip() for c in rows[0]]
        valid_quote_keys = ["quote", "text", "content", "è¯­å½•", "å¥å­", "å†…å®¹", "åè¨€"]
        valid_author_keys = ["author", "source", "from", "writer", "ä½œè€…", "å‡ºå¤„", "æ¥æº"]

        if any(k in first_row for k in valid_quote_keys + valid_author_keys):
            has_header = True
            header = rows[0]
            Logger.info(f"Detected Header: {header}", "CSV")

            for i, h in enumerate(first_row):
                if h in valid_quote_keys: 
                    q_idx = i
                elif h in valid_author_keys: 
                    a_idx = i
        else:
            Logger.info("No header detected, strictly using: Col 0=Author, Col 1=Quote", "CSV")

    data_rows = rows[1:] if has_header else rows
    rows_count = len(data_rows)
    
    if rows_count == 0:
        Logger.error("CSV has no data rows!")
        return 1
    Logger.success(f"Parsed {rows_count} valid data rows.")
    Logger.section("Picking Daily Sample")
    seed_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    rnd = random.Random(seed_date)
    
    if not data_rows:
        s_quote, s_author = "No data available", "System"
    else:
        sample_row = rnd.choice(data_rows)
        s_quote = sample_row[q_idx] if len(sample_row) > q_idx else "Unknown"
        s_author = sample_row[a_idx] if len(sample_row) > a_idx else "ä½šå"
    Logger.info(f"Selected: {s_quote[:20]}... -- {s_author}", "DAILY")

    ctx = {
        "repo": repo,
        "rows_count": rows_count,
        "diff_count": rows_count - old_row_count,
        "size_kb": int(csv_path.stat().st_size / 1024) + 1,
        "csv_sha": sha256_file(csv_path),
        "gen_utc": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC"),
        "gen_cn": datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S UTC+8"),
        "links": {
            "raw": f"https://raw.githubusercontent.com/{repo}/{branch}/{csv_rel}",
        }
    }

    Logger.section("Writing Content")
    new_readme = build_readme_content(ctx, {"quote": s_quote, "author": s_author})
    readme_path.write_text(new_readme, encoding="utf-8")
    Logger.success(f"README.md updated ({len(new_readme)} bytes written)")

    ctx['exec_time'] = time.time() - start_time
    generate_step_summary(ctx, []) 
    Logger.banner(f"JOB COMPLETED IN {ctx['exec_time']:.2f}s")
    
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except Exception:
        Logger.error("Unhandled Exception detected!")
        traceback.print_exc()
        sys.exit(1)
