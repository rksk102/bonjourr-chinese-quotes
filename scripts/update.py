import csv
import os
import sys
import random
import json
import time
import urllib.request
import urllib.error
import concurrent.futures
from datetime import datetime

TARGET_COUNT = 15
PRUNE_COUNT = 15
OUTPUT_FILE = "quotes.csv"
MAX_WORKERS = 4
REQUEST_TIMEOUT = 8
API_SOURCES = [
    {
        "name": "ä¸€è¨€ï¼ˆå®˜æ–¹-åŠ¨ç”»ï¼‰",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "a", "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆå®˜æ–¹-æ¼«ç”»ï¼‰",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "b", "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆå®˜æ–¹-æ–‡å­¦ï¼‰",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "d", "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆå®˜æ–¹-è¯—è¯ï¼‰",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "i", "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆå®˜æ–¹-å“²å­¦ï¼‰",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "k", "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "éŸ©å°éŸ©ï¼ˆä¸€è¨€é•œåƒï¼‰",
        "url": "https://api.vvhan.com/api/hitokoto",
        "params": {"type": "json"},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "å¤æŸ”ï¼ˆä¸€è¨€é•œåƒï¼‰",
        "url": "https://api.xygeng.cn/one",
        "params": {},
        "parser": lambda data: {"text": data.get("data", {}).get("content", "").strip(), "author": data.get("data", {}).get("origin", "ä½šå").strip()}
    }
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

class Log:
    RESET = '\033[0m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    @staticmethod
    def info(msg):
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"{Log.BLUE}[{timestamp}] â„¹ï¸  {msg}{Log.RESET}")
    @staticmethod
    def success(msg):
        print(f"{Log.GREEN}âœ… {msg}{Log.RESET}")
    @staticmethod
    def warning(msg):
        print(f"{Log.YELLOW}âš ï¸  {msg}{Log.RESET}")
    @staticmethod
    def error(msg):
        print(f"{Log.RED}âŒ {msg}{Log.RESET}")
    @staticmethod
    def group_start(title):
        print(f"::group::{title}")
    @staticmethod
    def group_end():
        print("::endgroup::")

class Stats:
    def __init__(self):
        self.api_calls = {source['name']: {'success': 0, 'fail': 0} for source in API_SOURCES}
    def record_success(self, name):
        if name in self.api_calls:
            self.api_calls[name]['success'] += 1
    def record_fail(self, name):
        if name in self.api_calls:
            self.api_calls[name]['fail'] += 1
stats_tracker = Stats()

def load_existing_quotes():
    """è¯»å– CSV è¿”å›é›†åˆå’Œåˆ—è¡¨"""
    Log.group_start("ğŸ“– æ­£åœ¨è¯»å–å†å²æ•°æ®")
    existing_set = set()
    existing_rows = []
    if not os.path.exists(OUTPUT_FILE):
        Log.warning(f"æ–‡ä»¶ {OUTPUT_FILE} ä¸å­˜åœ¨ï¼Œå°†åˆå§‹åŒ–æ–°æ–‡ä»¶")
        Log.group_end()
        return existing_set, existing_rows
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, fieldnames=['author', 'text'])
            for row in reader:
                text = row.get('text', '').strip()
                author = row.get('author', '').strip()
                if text == 'text' and author == 'author':
                    continue
                if text:
                    unique_key = f"{text}-{author}"
                    existing_set.add(unique_key)
                    existing_rows.append({'author': author, 'text': text})
        Log.info(f"è¯»å–æˆåŠŸ | å½“å‰æ€»æ•°: {len(existing_rows)}")
    except Exception as e:
        Log.error(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    Log.group_end()
    return existing_set, existing_rows

def prune_old_quotes(existing_rows, count_to_remove):
    """éšæœºåˆ é™¤æ—§æ•°æ®"""
    current_count = len(existing_rows)
    Log.group_start(f"âœ‚ï¸ æ•°æ®ä¿®å‰ª (ç›®æ ‡åˆ é™¤: {count_to_remove})")
    if current_count <= count_to_remove:
        Log.warning(f"å½“å‰æ¡æ•° ({current_count}) ä¸è¶³ï¼Œè·³è¿‡åˆ é™¤æ“ä½œ")
        Log.group_end()
        return existing_rows
    Log.info(f"æ­£åœ¨ä» {current_count} æ¡æ•°æ®ä¸­éšæœºç§»é™¤ {count_to_remove} æ¡...")
    random.shuffle(existing_rows)
    kept_rows = existing_rows[:current_count - count_to_remove]
    Log.success(f"ä¿®å‰ªå®Œæˆ | å‰©ä½™: {len(kept_rows)}")
    Log.group_end()
    return kept_rows

def fetch_one_quote(source_index=0):
    """å•æ¡æŠ“å–é€»è¾‘ï¼ˆå¸¦è¯¦ç»†é”™è¯¯è®°å½•ï¼‰"""
    start_time = time.time()
    for i in range(source_index, len(API_SOURCES)):
        source = API_SOURCES[i]
        try:
            params_str = "&".join([f"{k}={v}" for k, v in source["params"].items()])
            url = f"{source['url']}?{params_str}" if params_str else source['url']
            req = urllib.request.Request(url, headers=HEADERS)
            with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as response:
                data = json.loads(response.read().decode('utf-8'))
                parsed = source["parser"](data)
                text = parsed.get("text", "")
                author = parsed.get("author", "ä½šå")
                if text:
                    stats_tracker.record_success(source['name'])
                    return {
                        'text': text, 
                        'author': author, 
                        'source_name': source['name']
                    }
        except Exception as e:
            stats_tracker.record_fail(source['name'])
            pass
    return None

def draw_progress_bar(current, total, bar_length=30):
    percent = float(current) * 100 / total
    arrow = 'â–“' * int(percent / 100 * bar_length)
    spaces = 'â–‘' * (bar_length - len(arrow))
    sys.stdout.write(f"\r{Log.CYAN}ğŸš€ æ­£åœ¨æŠ“å–: [{arrow}{spaces}] {int(percent)}% ({current}/{total}){Log.RESET}")
    sys.stdout.flush()

def fetch_new_quotes(target, existing_set):
    """å¹¶å‘æŠ“å–ä¸»å¾ªç¯"""
    new_quotes = []
    consecutive_failures = 0
    MAX_CONSECUTIVE_FAILURES = 50
    print("\n")
    Log.info(f"å¼€å§‹ç½‘ç»œä½œä¸š | ç›®æ ‡æ–°å¢: {target} æ¡")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        while len(new_quotes) < target:
            needed = target - len(new_quotes)
            batch_size = min(needed + 2, MAX_WORKERS * 2) 
            futures = []
            for _ in range(batch_size):
                src_idx = random.randint(0, len(API_SOURCES) - 1)
                futures.append(executor.submit(fetch_one_quote, src_idx))
            round_has_success = False
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result:
                    u_key = f"{result['text']}-{result['author']}"
                    if u_key not in existing_set:
                        current_new_keys = {f"{q['text']}-{q['author']}" for q in new_quotes}
                        if u_key not in current_new_keys:
                            new_quotes.append(result)
                            existing_set.add(u_key)
                            round_has_success = True
                            if len(new_quotes) <= target:
                                draw_progress_bar(len(new_quotes), target)
            if not round_has_success:
                consecutive_failures += 1
            else:
                consecutive_failures = 0
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print()
                Log.error(f"è¿ç»­ {MAX_CONSECUTIVE_FAILURES} æ¬¡æŠ“å–å¤±è´¥ï¼Œæå‰ç»ˆæ­¢ã€‚")
                break
    print()
    Log.success(f"æŠ“å–ä½œä¸šå®Œæˆ | å®é™…è·å–: {len(new_quotes)} æ¡")
    return new_quotes

def rewrite_csv(all_quotes):
    """é‡å†™æ–‡ä»¶"""
    Log.group_start("ğŸ’¾ æ•°æ®å›å†™")
    try:
        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['author', 'text'], extrasaction='ignore')
            writer.writerows(all_quotes)
        Log.success(f"æ–‡ä»¶è¦†å†™æˆåŠŸ ({len(all_quotes)} æ¡è®°å½•)")
        Log.group_end()
        return True
    except Exception as e:
        Log.error(f"æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
        Log.group_end()
        return False

def generate_report(new_quotes, total_count):
    """ç”Ÿæˆæ¼‚äº®çš„ GitHub Summary"""
    summary_path = os.environ.get('GITHUB_STEP_SUMMARY')
    if not summary_path:
        return
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("# âœ¨ æ¯æ—¥è¯­å½•è‡ªåŠ¨æ›´æ–°æŠ¥å‘Š\n")
        f.write(f"> **â° è¿è¡Œæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)\n\n")
        f.write("### ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡\n")
        f.write("| ğŸ†• ä»Šæ—¥æ–°å¢ | ğŸ“‰ ä»Šæ—¥ç§»é™¤ | ğŸ“š å½“å‰åº“å­˜ |\n")
        f.write("| :---: | :---: | :---: |\n")
        f.write(f"| `{len(new_quotes)}` | `{PRUNE_COUNT}` | `{total_count}` |\n\n")
        f.write("<details><summary><b>ğŸ“¡ API è°ƒç”¨ç»Ÿè®¡ (ç‚¹æ­¤å±•å¼€)</b></summary>\n\n")
        f.write("| API åç§° | âœ… æˆåŠŸæ¬¡æ•° | âŒ å¤±è´¥/è·³è¿‡ |\n")
        f.write("| :--- | :---: | :---: |\n")
        for name, data in stats_tracker.api_calls.items():
            if data['success'] > 0 or data['fail'] > 0:
                f.write(f"| {name} | {data['success']} | {data['fail']} |\n")
        f.write("\n</details>\n\n")
        f.write("### ğŸ² æ–°å¢æ¡ç›®é¢„è§ˆ (Top 10)\n")
        f.write("| å†…å®¹ | ä½œè€…/å‡ºå¤„ | æ¥æºæ¸ é“ |\n")
        f.write("| :--- | :--- | :--- |\n")
        for q in new_quotes[:min(10, len(new_quotes))]:
            safe_text = q['text'].replace('|', '\\|').replace('\n', ' ')
            safe_author = q['author'].replace('|', '\\|')
            safe_source = q.get('source_name', 'æœªçŸ¥')
            f.write(f"| {safe_text} | {safe_author} | `{safe_source}` |\n")

if __name__ == "__main__":
    Log.info("è„šæœ¬å¯åŠ¨...")
    
    try:
        exist_set, exist_rows = load_existing_quotes()
        if len(exist_rows) >= PRUNE_COUNT:
            exist_rows = prune_old_quotes(exist_rows, PRUNE_COUNT)
            exist_set = {f"{r['text']}-{r['author']}" for r in exist_rows}
        new_data = fetch_new_quotes(TARGET_COUNT, exist_set)
        
        if len(new_data) > 0:
            final_data = exist_rows + new_data
            if rewrite_csv(final_data):
                generate_report(new_data, len(final_data))
            else:
                sys.exit(1)
        else:
            Log.warning("æœ¬æ¬¡è¿è¡Œæœªè·å–åˆ°ä»»ä½•æ–°æ•°æ®ã€‚")
            
    except KeyboardInterrupt:
        Log.error("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        Log.error(f"æœªæ•è·çš„å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
