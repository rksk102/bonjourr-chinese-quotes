import csv
import os
import sys
import random
import json
import time
import urllib.request
import urllib.error
import concurrent.futures
from datetime import datetime

TARGET_COUNT = 15
OUTPUT_FILE = "quotes.csv"
MAX_WORKERS = 3
REQUEST_TIMEOUT = 10

API_SOURCES = [
    {
        "name": "ä¸€è¨€ï¼ˆå®˜æ–¹ï¼‰",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": ["i", "l", "k"], "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆå›½é™…ç‰ˆï¼‰",
        "url": "https://international.v1.hitokoto.cn/",
        "params": {"c": ["i", "l", "k"], "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆCNé•œåƒï¼‰",
        "url": "https://cn.hitokoto.cn/",
        "params": {"c": ["i", "l", "k"], "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆå¤‡ç”¨åŸŸåï¼‰",
        "url": "https://sentence-api.qpchan.com/",
        "params": {"c": ["i", "l", "k"], "encode": "json", "min_length": 5, "max_length": 30},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€ï¼ˆPHPç‰ˆï¼‰",
        "url": "https://hitokoto.cn/api.php",
        "params": {"c": "i", "encode": "json"},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€è¯—è¯",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "k", "encode": "json"},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€æ–‡å­¦",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "l", "encode": "json"},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä¸€è¨€æ–‡è¨€",
        "url": "https://v1.hitokoto.cn/",
        "params": {"c": "d", "encode": "json"},
        "parser": lambda data: {"text": data.get("hitokoto", "").strip(), "author": data.get("from", "ä½šå").strip()}
    },
    {
        "name": "ä»Šæ—¥è¯—è¯",
        "url": "https://v2.jinrishici.com/one.json",
        "params": {},
        "parser": lambda data: {"text": data.get("data", {}).get("content", "").strip(), "author": data.get("data", {}).get("origin", {}).get("author", "ä½šå").strip()}
    },
    {
        "name": "å¤è¯—è¯API",
        "url": "https://api.gushi.ci/all.json",
        "params": {},
        "parser": lambda data: {"text": data[0].get("content", "").strip() if isinstance(data, list) and len(data) > 0 else "", "author": data[0].get("origin", {}).get("author", "ä½šå").strip() if isinstance(data, list) and len(data) > 0 else "ä½šå"}
    },
    {
        "name": "çˆ±è¯å»ºè¯—è¯",
        "url": "https://ciapi.xygeng.cn/one",
        "params": {},
        "parser": lambda data: {"text": data.get("content", "").strip(), "author": data.get("author", "").strip() if data.get("author") else "ä½šå"}
    },
    {
        "name": "éšæœºå¥å­",
        "url": "https://api.xygeng.cn/one",
        "params": {},
        "parser": lambda data: {"text": data.get("text", "").strip(), "author": data.get("author", "ä½šå").strip()}
    },
    {
        "name": "å¥å­è¿·API",
        "url": "https://api.juzimi.com/api/random",
        "params": {},
        "parser": lambda data: {"text": data.get("content", "").strip(), "author": data.get("author", "å¥å­è¿·").strip()}
    },
    {
        "name": "ä¸€è¨€ä»£ç†",
        "url": "https://api.vvhan.com/api/ä¸€è¨€",
        "params": {},
        "parser": lambda data: {"text": data.get("data", {}).get("hitokoto", "").strip(), "author": data.get("data", {}).get("from", "ä½šå").strip()}
    },
    {
        "name": "åŠ±å¿—åè¨€",
        "url": "https://api.oick.cn/dutang/api.php",
        "params": {},
        "parser": lambda data: {"text": data.get("text", "").strip(), "author": data.get("author", "ä½šå").strip()}
    },
    {
        "name": "åäººåè¨€",
        "url": "https://api.oick.cn/mingyan/api.php",
        "params": {},
        "parser": lambda data: {"text": data.get("text", "").strip(), "author": data.get("author", "ä½šå").strip()}
    },
    {
        "name": "å¿ƒçµé¸¡æ±¤",
        "url": "https://api.oick.cn/yulu/api.php",
        "params": {},
        "parser": lambda data: {"text": data.get("text", "").strip(), "author": data.get("author", "ä½šå").strip()}
    },
    {
        "name": "æ–‡è‰ºå¥å­",
        "url": "https://api.oick.cn/wenyi/api.php",
        "params": {},
        "parser": lambda data: {"text": data.get("text", "").strip(), "author": data.get("author", "ä½šå").strip()}
    },
    {
        "name": "éšæœºæƒ…è¯",
        "url": "https://api.uomg.com/api/rand.qinghua",
        "params": {},
        "parser": lambda data: {"text": data.get("text", "").strip(), "author": "æƒ…è¯API"}
    },
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def log(message, type='info'):
    timestamp = datetime.now().strftime("%H:%M:%S")
    if type == 'error':
        print(f"::error::{message}")
    elif type == 'warning':
        print(f"::warning::{message}")
    else:
        print(f"[{timestamp}] {message}")

def load_existing_quotes():
    """
    è¯»å–ç°æœ‰çš„ CSV æ–‡ä»¶ï¼Œè¿”å›å»é‡é›†åˆï¼ˆç”¨äºåç»­å»é‡ï¼‰
    """
    existing_set = set()
    existing_count = 0
    
    if not os.path.exists(OUTPUT_FILE):
        log(f"ğŸ“ æ–‡ä»¶ {OUTPUT_FILE} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶", 'info')
        return existing_set, 0
    
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                text = row.get('text', '').strip()
                author = row.get('author', '').strip()
                if text:
                    unique_key = f"{text}-{author}"
                    existing_set.add(unique_key)
                    existing_count += 1
        log(f"ğŸ“š å·²åŠ è½½ {existing_count} æ¡å†å²è¯­å½•", 'info')
    except Exception as e:
        log(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶", 'warning')
    
    return existing_set, existing_count

def fetch_one_quote(source_index=0):
    """
    ä»æŒ‡å®šç´¢å¼•çš„ API æºè·å–ä¸€æ¡è¯­å½•ï¼Œå¤±è´¥åˆ™å°è¯•ä¸‹ä¸€ä¸ªæº
    """
    for i in range(source_index, len(API_SOURCES)):
        source = API_SOURCES[i]
        try:
            params_str = "&".join([f"{k}={v}" for k, v in source["params"].items()])
            url = f"{source['url']}?{params_str}" if params_str else source['url']
            
            req = urllib.request.Request(url, headers=HEADERS)
            with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as response:
                data = json.loads(response.read().decode('utf-8'))
                
                parsed = source["parser"](data)
                text = parsed.get("text", "")
                author = parsed.get("author", "ä½šå")
                
                if text:
                    return {'text': text, 'author': author}
        except Exception as e:
            pass
    
    return None

def fetch_new_quotes(count, existing_set):
    """
    è·å–æ–°çš„è¯­å½•ï¼ˆä¸é‡å¤çš„ï¼‰
    """
    new_quotes = []
    consecutive_failures = 0
    MAX_FAILURES = 20
    
    log(f"ğŸš€ å¼€å§‹è·å– {count} æ¡æ–°è¯­å½•...", 'info')
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        while len(new_quotes) < count:
            needed = count - len(new_quotes)
            batch_size = min(needed, MAX_WORKERS * 2)
            
            source_index = random.randint(0, len(API_SOURCES) - 1)
            futures = [executor.submit(fetch_one_quote, source_index) for _ in range(batch_size)]
            
            round_success = 0
            
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result:
                    unique_key = f"{result['text']}-{result['author']}"
                    if unique_key not in existing_set
                        new_keys = {f"{q['text']}-{q['author']}" for q in new_quotes}
                        if unique_key not in new_keys:
                            existing_set.add(unique_key)
                            new_quotes.append(result)
                            round_success += 1
                            sys.stdout.write(f"\r   è¿›åº¦: {len(new_quotes)}/{count}")
                            sys.stdout.flush()
            
            if round_success == 0:
                consecutive_failures += 1
                log(f"âš ï¸ ç¬¬ {consecutive_failures} æ¬¡å°è¯•æœªè·å–åˆ°æ–°æ•°æ®", 'warning')
            else:
                consecutive_failures = 0
            
            if consecutive_failures >= MAX_FAILURES:
                log(f"âŒ è¿ç»­ {MAX_FAILURES} æ¬¡å¤±è´¥ï¼Œç»ˆæ­¢è·å–", 'error')
                break
    
    elapsed = time.time() - start_time
    print()
    log(f"âœ… è·å–å®Œæˆï¼æ–°å¢ {len(new_quotes)} æ¡ï¼Œè€—æ—¶: {elapsed:.2f} ç§’", 'info')
    return new_quotes

def append_to_csv(new_quotes):
    """
    å°†æ–°è¯­å½•è¿½åŠ åˆ° CSV æ–‡ä»¶
    """
    print("::group::ğŸ’¾ è¿½åŠ æ–°è¯­å½•åˆ° CSV")
    try:
        with open(OUTPUT_FILE, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['text', 'author'])
            
            if os.stat(OUTPUT_FILE).st_size == 0:
                writer.writeheader()
            
            writer.writerows(new_quotes)
        
        print(f"âœ… å·²è¿½åŠ  {len(new_quotes)} æ¡åˆ° {OUTPUT_FILE}")
        print("æ–°å¢çš„è¯­å½•é¢„è§ˆ:")
        for i, q in enumerate(new_quotes[:3]):
            print(f"  {i+1}. {q['text']}")
        print("::endgroup::")
        return True
    except Exception as e:
        log(f"ä¿å­˜å¤±è´¥: {e}", 'error')
        return False

def generate_summary(new_quotes, total_count):
    summary_path = os.environ.get('GITHUB_STEP_SUMMARY')
    if not summary_path:
        return

    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ“… æ¯æ—¥è¯­å½•æ›´æ–°æŠ¥å‘Š\n\n")
        f.write(f"**â° æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} \n\n")
        f.write(f"**ğŸ†• ä»Šæ—¥æ–°å¢**: `{len(new_quotes)}` æ¡ \n\n")
        f.write(f"**ğŸ“š æ€»è®¡**: `{total_count}` æ¡ \n\n")
        
        if len(new_quotes) > 0:
            f.write("### âœ¨ ä»Šæ—¥æ–°å¢é¢„è§ˆ\n")
            f.write("| å†…å®¹ | å‡ºå¤„ |\n")
            f.write("| :--- | :--- |\n")
            for q in new_quotes[:min(5, len(new_quotes))]:
                safe_text = q['text'].replace('|', '\\|')
                safe_author = q['author'].replace('|', '\\|')
                f.write(f"| {safe_text} | {safe_author} |\n")

if __name__ == "__main__":
    start_time = time.time()
    
    try:
        existing_set, existing_count = load_existing_quotes()
        
        new_quotes = fetch_new_quotes(TARGET_COUNT, existing_set)
        
        if len(new_quotes) > 0:
            if append_to_csv(new_quotes):
                total_count = existing_count + len(new_quotes)
                generate_summary(new_quotes, total_count)
                log("ğŸ‰ ä»»åŠ¡å®Œæˆï¼", 'info')
            else:
                sys.exit(1)
        else:
            log("âš ï¸ æ²¡æœ‰è·å–åˆ°æ–°è¯­å½•ï¼Œæ–‡ä»¶ä¿æŒä¸å˜", 'warning')
            
    except Exception as e:
        log(f"ä¸¥é‡é”™è¯¯: {e}", 'error')
        sys.exit(1)
